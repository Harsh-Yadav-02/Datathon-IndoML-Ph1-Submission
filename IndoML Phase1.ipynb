{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb756b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def read_jsonl(file_path, nrows=None):\n",
    "    return pd.read_json(file_path, lines=True, nrows=nrows)\n",
    "\n",
    "\n",
    "train_data = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/LLM_data/attribute_train.data')\n",
    "train_solution = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/LLM_data/attribute_train.solution')\n",
    "test_data = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/LLM_data/attribute_test.data')\n",
    "#test_solution = read_jsonl('./data/attribute_test.solution', nrows=200)\n",
    "val_data = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/LLM_data/attribute_val.data')\n",
    "val_solution = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/LLM_data/attribute_val.solution')\n",
    "\n",
    "def preprocess_data(data, solution=None):\n",
    "    if solution is not None:\n",
    "        merged = pd.merge(data, solution, on='indoml_id')\n",
    "        merged['input_text'] = merged.apply(lambda row: f\"title: {row['title']} store: {row['store']} details_Manufacturer: {row['details_Manufacturer']}\", axis=1)\n",
    "        merged['target_text'] = merged.apply(lambda row: f\"details_Brand: {row['details_Brand']} L0_category: {row['L0_category']} L1_category: {row['L1_category']} L2_category: {row['L2_category']} L3_category: {row['L3_category']} L4_category: {row['L4_category']}\", axis=1)\n",
    "        return merged[['input_text', 'target_text']]\n",
    "    \n",
    "    else:\n",
    "        data['input_text'] = data.apply(lambda row: f\"title: {row['title']} store: {row['store']} details_Manufacturer: {row['details_Manufacturer']}\", axis=1)\n",
    "        return data[['input_text']]\n",
    "\n",
    "\n",
    "train_processed = preprocess_data(train_data, train_solution)\n",
    "test_processed = preprocess_data(test_data)\n",
    "val_processed = preprocess_data(val_data, val_solution)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_processed)\n",
    "test_dataset = Dataset.from_pandas(test_processed)\n",
    "val_dataset = Dataset.from_pandas(val_processed)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    #'test': test_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299db04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f001dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54130fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d3f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('./')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=500,\n",
    "    per_device_eval_batch_size=500,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step: {state.global_step}\")\n",
    "            for key, value in logs.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "print(f\"Validation Loss: {val_results['eval_loss']}\")\n",
    "\n",
    "#test_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "#print(f\"Test Loss: {test_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./fine_tuned_t5_full_50epochsdp')\n",
    "tokenizer.save_pretrained('./fine_tuned_t5_full_50epochsdp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68467a",
   "metadata": {},
   "source": [
    "# Next time run from here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87593e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('./fine_tuned_t5_full_50epochsdp').to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('./fine_tuned_t5_full_50epochsdp')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_data = test_dataset['input_text']\n",
    "#test_label = test_dataset['target_text']\n",
    "\n",
    "def generate_text(inputs):\n",
    "    inputs = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=352)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    cleaned_texts = [clean_repeated_patterns(text) for text in generated_texts]\n",
    "    return cleaned_texts\n",
    "\n",
    "def extract_details(text):\n",
    "    pattern = r'details_Brand: (.*?) L0_category: (.*?) L1_category: (.*?) L2_category: (.*?) L3_category: (.*?) L4_category: (.*)'\n",
    "    match = re.match(pattern, text)\n",
    "    if match:\n",
    "        return tuple(item if item is not None else 'na' for item in match.groups())\n",
    "    return 'na', 'na', 'na', 'na', 'na', 'na'\n",
    "\n",
    "def clean_repeated_patterns(text):\n",
    "    cleaned_data = text.split(' L4_category')[0] \n",
    "    return cleaned_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "generated_details = []\n",
    "target_details = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n",
    "    batch_inputs = test_data[i:i+batch_size]\n",
    "    #batch_labels = test_label[i:i+batch_size] #you are not going to have this\n",
    "    \n",
    "    generated_texts = generate_text(batch_inputs)\n",
    "    \n",
    "    for generated_text in generated_texts:\n",
    "        generated_details.append(extract_details(generated_text))\n",
    "\n",
    "print('Generated info extracted.............')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43288da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "categories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\n",
    "\n",
    "with open('full_50epochs.predict', 'w') as file:\n",
    "\n",
    "    for indoml_id, details in enumerate(generated_details):\n",
    "        result = {\"indoml_id\": indoml_id}\n",
    "        for category, value in zip(categories, details):\n",
    "            result[category] = value\n",
    "        \n",
    "        file.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e666968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "file_to_zip = 'full_50epochs.predict'\n",
    "zip_file_name = 'full_50epochs.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "    zipf.write(file_to_zip, arcname=file_to_zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
